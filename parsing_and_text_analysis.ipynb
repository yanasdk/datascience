{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсинг новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8cd6179df4b8>:29: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  df = json_normalize(df['items'].apply(json.dumps).apply(json.loads))\n"
     ]
    }
   ],
   "source": [
    "import datetime as DT\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "\n",
    "start_date = DT.datetime(2010, 1, 1)\n",
    "end_date = DT.datetime(2020, 1, 1)\n",
    "\n",
    "dates = pd.date_range(\n",
    "    min(start_date, end_date),\n",
    "    max(start_date, end_date), \n",
    "    freq = 'MS').strftime('%m/%d/%Y').tolist()\n",
    "\n",
    "dates2 = dates[1:]\n",
    "\n",
    "lst = []\n",
    "k = 100\n",
    "for start, end in zip(dates, dates2):\n",
    "    for i in range(0, 5000, 100):\n",
    "        url = \"https://www.rbc.ru/v10/search/ajax/?project=quote&dateFrom={}&dateTo={}&offset={}&limit={}&query=%D0%A0%D0%91%D0%9A\".format(start, end, i ,k)\n",
    "        response = urlopen(url)\n",
    "        data = json.loads(response.read())\n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        df = json_normalize(df['items'].apply(json.dumps).apply(json.loads))\n",
    "        lst.append(df)\n",
    "\n",
    "df_news = pd.concat(lst, ignore_index=True)\n",
    "\n",
    "#удаление лишней информации и дубликатов\n",
    "df_news = df_news[[\"anons\", \"category\", \"publish_date\", \"title\"]]\n",
    "df_news = df_news.drop_duplicates(['title'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка и агрегация новостных заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pymystem3 import Mystem\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "pd.set_option('display.max_columns', 10000) \n",
    "\n",
    "data = pd.read_csv(\"/Users/yanasidikova/data.csv\")\n",
    "data = data[[\"date\", \"datetime\", \"title\", \"category\"]]\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data2)):\n",
    "    data[\"title\"][i] = str(data[\"title\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ru(file_text):\n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "    tokens = [i for i in tokens if (i not in string.punctuation)]\n",
    "    stop_words = stopwords.words('russian')\n",
    "    stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', '–', 'к', 'на', '...', '``', \"''\", \",\", \"»\", \"«\", \"из-за\", \"свой\", \"num\", \"без\", \"ко\"])\n",
    "    tokens = [i for i in tokens if (i not in stop_words)]\n",
    "    tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
    "    return tokens\n",
    "\n",
    "m = Mystem()\n",
    "def lemmatize_sentence(text):\n",
    "    lemmas = m.lemmatize(text)\n",
    "    return \"\".join(lemmas).strip()\n",
    "\n",
    "\n",
    "def preprocessing(df):\n",
    "    df2 = df\n",
    "    df2[\"text\"] = \"\"\n",
    "    for i in range(0, len(df2)):\n",
    "        df2[\"title\"].iloc[i] = re.sub('(\\d)+', 'NUM', df2[\"title\"].iloc[i])\n",
    "\n",
    "        df2[\"text\"].iloc[i] = lemmatize_sentence(df2[\"title\"].iloc[i].lower())\n",
    "        #df2[\"text\"].iloc[i] = tokenize_ru(df2[\"text\"].iloc[i])\n",
    "        #print(i)\n",
    "    return df2\n",
    "\n",
    "def aggregation(df, var):\n",
    "    grouped_df = df.groupby(var)\n",
    "    grouped_lists = grouped_df[\"text\"].agg(lambda column: \" \".join(column))\n",
    "    grouped_lists = grouped_lists.reset_index(name=\"text2\")\n",
    "    grouped_lists[\"numwords\"] = \"\"\n",
    "    for i in range(0, len(grouped_lists)):\n",
    "        grouped_lists[\"text2\"].iloc[i] = tokenize_ru(grouped_lists[\"text2\"].iloc[i])\n",
    "        grouped_lists[\"numwords\"].iloc[i] = len(grouped_lists[\"text2\"].iloc[i])\n",
    "        #print(i)\n",
    "    return grouped_lists\n",
    "\n",
    "\n",
    "def counting_words(df):\n",
    "    text = df['text2'].tolist()\n",
    "    flat_text = []\n",
    "    for sublist in text:\n",
    "        for item in sublist:\n",
    "            flat_text.append(item)\n",
    "    counts = Counter(flat_text)\n",
    "    numwords = pd.DataFrame([{\"word\": word, \"count\": count} for word, count in counts.items()])\n",
    "    return numwords\n",
    "\n",
    "def index_counter(df, wordlist, t):\n",
    "    wordlist = wordlist[[\"word\", \"rate\"]]\n",
    "    dic = wordlist.set_index([\"word\"]).to_dict()[\"rate\"]\n",
    "    rating = []\n",
    "    value = 0\n",
    "    for row in df['text2']:\n",
    "        for word in row:\n",
    "            value += dic.get(word,0)\n",
    "        rating.append(value)\n",
    "        value = 0\n",
    "    index = pd.DataFrame()\n",
    "    index['t'] = df[t]\n",
    "    index['num'] = df['numwords']\n",
    "    index['indexv'] = rating\n",
    "    index['ind'] = ''\n",
    "    index['ind'] = index.groupby(['t'], group_keys=False).apply(lambda x: x.indexv/x.num)\n",
    "    index = index[['t', 'ind']]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics = data_cleaned[data_cleaned.category == 'Политика']\n",
    "finance = data_cleaned[(data_cleaned.category == 'Финансы') | (data_cleaned.category == 'Экономика')]\n",
    "business = data_cleaned[data_cleaned.category == 'Бизнес']data = data[['date', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_daily = aggregation(data_cleaned, \"date\")\n",
    "data_hourly = aggregation(data_cleaned, \"datetime\")\n",
    "pol_daily = aggregation(politics, \"date\")\n",
    "pol_hourly = aggregation(politics, \"datetime\")\n",
    "fin_daily = aggregation(finance, \"date\")\n",
    "fin_hourly = aggregation(finance, \"datetime\")\n",
    "bus_daily = aggregation(business, \"date\")\n",
    "bus_hourly = aggregation(business, \"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание словарей и расчет индексов тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = counting_words(data_daily)\n",
    "wordlist_pol = counting_words(pol_daily)\n",
    "wordlist_fin = counting_words(fin_daily)\n",
    "wordlist_bus = counting_words(bus_daily)\n",
    "\n",
    "pos_d = pd.read_csv(\"/Users/yanasidikova/positive_news_daily.csv\")\n",
    "pos_h = pd.read_csv(\"/Users/yanasidikova/hrl_pos.csv\")\n",
    "pos_d = pos_d.drop_duplicates()\n",
    "pos_h = pos_h.drop_duplicates()\n",
    "pos_d = pos_d[\"title\"].tolist()\n",
    "pos_h = pos_hourly[\"title\"].tolist()\n",
    "\n",
    "positive_daily = data_cleaned[data_cleaned['title'].isin(pos_d)]\n",
    "positive_hourly = data_cleaned[data_cleaned['title'].isin(pos_h)]\n",
    "positive_daily['text2'] = positive_daily['text']\n",
    "positive_hourly['text2'] = positive_hourly['text']\n",
    "pos_pol_daily = positive_daily[positive_daily.category == 'Политика']\n",
    "pos_pol_hourly = positive_hourly[positive_hourly.category == 'Политика']\n",
    "pos_fin_daily = positive_daily[(positive_daily.category == 'Финансы') | (positive_daily.category == 'Экономика')]\n",
    "pos_fin_hourly = positive_hourly[(positive_hourly.category == 'Финансы') | (positive_hourly.category == 'Экономика')]\n",
    "pos_bus_daily = positive_daily[positive_daily.category == 'Бизнес']\n",
    "pos_bus_hourly = positive_hourly[positive_hourly.category == 'Бизнес']\n",
    "\n",
    "positive_daily = aggregation(positive_daily, \"date\")\n",
    "positive_hourly = aggregation(positive_hourly, \"datetime\")\n",
    "pos_pol_daily = aggregation(pos_pol_daily, \"date\")\n",
    "pos_pol_hourly = aggregation(pos_pol_hourly, \"datetime\")\n",
    "pos_fin_daily = aggregation(pos_fin_daily, \"date\")\n",
    "pos_fin_hourly = aggregation(pos_fin_hourly, \"datetime\")\n",
    "pos_bus_daily = aggregation(pos_bus_daily, \"date\")\n",
    "pos_bus_hourly = aggregation(pos_bus_hourly, \"datetime\")\n",
    "\n",
    "pos_wordlist_d = counting_words(positive_daily)\n",
    "pos_wordlist_h = counting_words(positive_hourly)\n",
    "pos_wordlist_pol_d = counting_words(pos_pol_daily)\n",
    "pos_wordlist_pol_h = counting_words(pos_pol_hourly)\n",
    "pos_wordlist_fin_d = counting_words(pos_fin_daily)\n",
    "pos_wordlist_fin_h = counting_words(pos_fin_hourly)\n",
    "pos_wordlist_bus_d = counting_words(pos_bus_daily)\n",
    "pos_wordlist_bus_h = counting_words(pos_bus_hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_d = pd.merge(wordlist,\n",
    "                 pos_wordlist_d,\n",
    "                 on='word', \n",
    "                 how='left').replace(np.nan, 0)\n",
    "wordlist_h = pd.merge(wordlist,\n",
    "                 pos_wordlist_h,\n",
    "                 on='word', \n",
    "                 how='left').replace(np.nan, 0)\n",
    "wordlist_pol_d = pd.merge(wordlist_pol,\n",
    "                 pos_wordlist_pol_d,\n",
    "                 on='word', \n",
    "                 how='left').replace(np.nan, 0)\n",
    "wordlist_pol_h = pd.merge(wordlist_pol,\n",
    "                 pos_wordlist_pol_h,\n",
    "                 on='word', \n",
    "                 how='left').replace(np.nan, 0)\n",
    "wordlist_fin_d = pd.merge(wordlist_fin,\n",
    "                 pos_wordlist_fin_d,\n",
    "                 on='word', \n",
    "                 how='left').replace(np.nan, 0)\n",
    "wordlist_fin_h = pd.merge(wordlist_fin,\n",
    "                 pos_wordlist_fin_h,\n",
    "                 on='word', \n",
    "                 how='left').replace(np.nan, 0)\n",
    "wordlist_bus_d = pd.merge(wordlist_bus,\n",
    "                 pos_wordlist_bus_d,\n",
    "                 on='word', \n",
    "                 how='left').replace(np.nan, 0)\n",
    "wordlist_bus_h = pd.merge(wordlist_bus,\n",
    "                 pos_wordlist_bus_h,\n",
    "                 on='word', \n",
    "                 how='left').replace(np.nan, 0)\n",
    "\n",
    "for df in (wordlist_d, wordlist_h, wordlist_pol_d, wordlist_pol_h, wordlist_fin_d, wordlist_fin_h, wordlist_bus_d, \\\n",
    "           wordlist_bus_h):\n",
    "    df['rate'] = df.groupby(['word'], group_keys=False).apply(lambda g: g.count_y/g.count_x)\n",
    "    \n",
    "for df in (wordlist_d, wordlist_h):\n",
    "    df = df[(df.count_x >= 400) & (df.count_y >= 10)]\n",
    "for df in (wordlist_pol_d, wordlist_pol_h, wordlist_fin_d, wordlist_fin_h, wordlist_bus_d, wordlist_bus_h):\n",
    "    df = df[(df.count_x >= 100) & (df.count_y >= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ind = index_counter(data_daily, wordlist_d, \"date\")\n",
    "d_ind_p = index_counter(pol_daily, wordlist_pol_d, \"date\")\n",
    "d_ind_f = index_counter(fin_daily, wordlist_fin_d, \"date\")\n",
    "d_ind_b = index_counter(bus_daily, wordlist_bus_d, \"date\")\n",
    "h_ind = index_counter(data_hourly, wordlist_h, \"datetime\")\n",
    "h_ind_p = index_counter(pol_hourly, wordlist_pol_h, \"datetime\")\n",
    "h_ind_f = index_counter(fin_hourly, wordlist_fin_h, \"datetime\")\n",
    "h_ind_b = index_counter(bus_hourly, wordlist_bus_h, \"datetime\")\n",
    "\n",
    "day = pd.DataFrame()\n",
    "day['t'] = d_ind['t']\n",
    "day['ind'] = d_ind['ind']\n",
    "day = pd.merge(day,\n",
    "                 d_ind_p[['t', 'ind']],\n",
    "                 on='t', \n",
    "                 how='left', suffixes=('','_p')).replace(np.nan, 0)\n",
    "day = pd.merge(day,\n",
    "                 d_ind_f[['t', 'ind']],\n",
    "                 on='t', \n",
    "                 how='left', suffixes=('','_f')).replace(np.nan, 0)\n",
    "day = pd.merge(day,\n",
    "                 d_ind_b[['t', 'ind']],\n",
    "                 on='t', \n",
    "                 how='left', suffixes=('','_b')).replace(np.nan, 0)\n",
    "day.to_csv(\"daily4M.csv\")\n",
    "\n",
    "hour = pd.DataFrame()\n",
    "hour['t'] = h_ind['t']\n",
    "hour['ind'] = h_ind['ind']\n",
    "hour = pd.merge(hour,\n",
    "                 h_ind_p[['t', 'ind']],\n",
    "                 on='t', \n",
    "                 how='left', suffixes=('','_p')).replace(np.nan, 0)\n",
    "hour = pd.merge(hour,\n",
    "                 h_ind_f[['t', 'ind']],\n",
    "                 on='t', \n",
    "                 how='left', suffixes=('','_f')).replace(np.nan, 0)\n",
    "hour = pd.merge(hour,\n",
    "                 h_ind_b[['t', 'ind']],\n",
    "                 on='t', \n",
    "                 how='left', suffixes=('','_b')).replace(np.nan, 0)\n",
    "hour.to_csv(\"hourly4M.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
